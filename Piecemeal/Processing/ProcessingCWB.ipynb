{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 匯入函數\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import progressbar as bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 資料篩選\n",
    "\n",
    "def Filter_data(path):\n",
    "\n",
    "    column = ['time','longitude','latitude','scale','depth']\n",
    "\n",
    "    CWBdata = pd.read_csv(path, encoding='big5')\n",
    "    Sortdata = pd.DataFrame(np.array(CWBdata[:len(CWBdata)].iloc[::-1]), columns=column)\n",
    "\n",
    "    frequencytotal = np.unique(Sortdata['scale'], return_counts=True)\n",
    "    frequencydict = dict(zip(frequencytotal[1], frequencytotal[0]))\n",
    "\n",
    "    filterscale = Sortdata['scale'] >= frequencydict.get(max(frequencytotal[1]))\n",
    "    filterdata = pd.DataFrame(np.array(Sortdata[filterscale]), columns=column).dropna(axis = 0)\n",
    "    \n",
    "    return filterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Filter_data('D:/Dataset/Earthquake/Original/CWB_593634_19990101_20201231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 時間拆分\n",
    "\n",
    "def Data_time_Split(data):\n",
    "    \n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    data['year'] = pd.to_datetime(data['time']).dt.year\n",
    "    data['month'] = pd.to_datetime(data['time']).dt.month\n",
    "    data['day'] = pd.to_datetime(data['time']).dt.day\n",
    "    \n",
    "    data = data[['time','year','month','day','longitude','latitude','scale','depth']]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data_time_Split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 規模概率計算\n",
    "\n",
    "def A1516(Lower,Upper):\n",
    "    \n",
    "    A1516array,y = np.empty(len(data)),0\n",
    "    total = data.time.iloc[-1]-data.time.iloc[0]\n",
    "    mean = total.total_seconds()/len(data[(data.scale>=Lower)&(data.scale<Upper)])\n",
    "\n",
    "    for x in np.arange(len(data)):\n",
    "\n",
    "        if (data.scale[x]>=Lower)&(data.scale[x]<Upper):\n",
    "\n",
    "            y=x\n",
    "\n",
    "        A1516 = (data.time[x]-data.time[y]).total_seconds()/mean\n",
    "        A1516array[x] = A1516\n",
    "        \n",
    "    return A1516array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 每次機率\n",
    "\n",
    "def Repeat_operation_Each(i):\n",
    "\n",
    "    Probability = np.empty(shape=(int((max(data.scale)-min(data.scale))*(1/i)+1),len(data)))\n",
    "\n",
    "    for x in bar.progressbar(np.arange(int((max(data.scale)-min(data.scale))*(1/i)+1))):\n",
    "        \n",
    "        if len(data[(data.scale>=min(data.scale)+i*x)&(data.scale<min(data.scale)+i*(x+1))])!=0:\n",
    "\n",
    "            Probability[x] = A1516(min(data.scale)+i*x,min(data.scale)+i*(x+1))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            Probability[x] = 0\n",
    "            \n",
    "    Probabilitys = pd.DataFrame(Probability.T,columns=pd.DataFrame(Probability.T).columns*101+1516)\n",
    "    Probabilitys.index = np.arange(len(Probabilitys))\n",
    "        \n",
    "    return Probabilitys[Probabilitys.columns[(Probabilitys==0).all()==False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (59 of 59) |########################| Elapsed Time: 0:15:37 Time:  0:15:37\n"
     ]
    }
   ],
   "source": [
    "Probabilityeach = Repeat_operation_Each(0.1) # 控制間隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 每天機率\n",
    "\n",
    "def Repeat_operation_Day(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    Probabilityeach['year'] = data['year']\n",
    "    Probabilityeach['month'] = data['month']\n",
    "    Probabilityeach['day'] = data['day']\n",
    "\n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "    \n",
    "    P = pd.DataFrame()\n",
    "        \n",
    "    for year in bar.progressbar(years):\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "                \n",
    "                P = pd.concat([P,np.mean(Probabilityeach[(Probabilityeach['year']==year)&(Probabilityeach['month']==month)&(Probabilityeach['day']==day)])],axis=1)\n",
    "                \n",
    "    Probability = P.T\n",
    "    Probability.index = np.arange(len(Probability))\n",
    "    \n",
    "    return Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (22 of 22) |########################| Elapsed Time: 0:06:36 Time:  0:06:36\n"
     ]
    }
   ],
   "source": [
    "Probabilityday = Repeat_operation_Day(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 歷史古騰堡參數\n",
    "\n",
    "def Gutenberg_Richter_total(scale):\n",
    "    \n",
    "    intercept,coef  = [],[]\n",
    "    \n",
    "    frequencytotal = np.unique(scale, return_counts=True)\n",
    "    table = pd.DataFrame(frequencytotal, index=['scale', 'frequency']).T\n",
    "    table['cumsum'] = table['frequency'].iloc[::-1].cumsum()\n",
    "    table['log'] = np.log10(table['cumsum'])\n",
    "    x = np.array([table['scale']]).T\n",
    "    y = np.array([table['log']]).T\n",
    "    LR = LinearRegression().fit(x, y)\n",
    "    \n",
    "    intercept.append(LR.intercept_[0])\n",
    "    coef.append(-LR.coef_[0][0])\n",
    "    \n",
    "    return intercept,coef,table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRtotal = Gutenberg_Richter_total(data['scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([GRtotal[2].iloc[:,0:1],GRtotal[2].iloc[:,2:3]],axis=1).to_excel('C:/Users/btea4/OneDrive/桌面/GRtotal2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 區間古騰堡參數\n",
    "\n",
    "def Gutenberg_Richter_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    intercept,coef,GRloss,scalemaxexpected  = [],[],[],[]\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "            \n",
    "                scale = data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale']\n",
    "                \n",
    "                frequencytemporary = np.unique(scale, return_counts=True)\n",
    "                table = pd.DataFrame(frequencytemporary, index=['scale', 'frequency']).T\n",
    "                table['cumsum'] = table['frequency'].iloc[::-1].cumsum()\n",
    "                table['log'] = np.log10(table['cumsum'])\n",
    "                x = np.array([table['scale']]).T\n",
    "                y = np.array([table['log']]).T\n",
    "                LR = LinearRegression().fit(x, y)\n",
    "\n",
    "                loss = sum((GRtotal[0]-GRtotal[1]*scale)-(LR.intercept_[0]-LR.coef_[0][0]*scale))/len(scale)\n",
    "                \n",
    "                expected = LR.intercept_[0]/-LR.coef_[0][0]\n",
    "\n",
    "                intercept.append(LR.intercept_[0])\n",
    "                coef.append(-LR.coef_[0][0])\n",
    "                GRloss.append(loss)\n",
    "                scalemaxexpected.append(expected)\n",
    "\n",
    "    return intercept,coef,GRloss,scalemaxexpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRinterval = Gutenberg_Richter_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 區間平均規模\n",
    "\n",
    "def scale_mean_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    scalemean = []\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "                \n",
    "                scale = np.mean(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale'])\n",
    "                scalemean.append(scale)\n",
    "            \n",
    "    return scalemean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalemean = scale_mean_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 區間最大規模\n",
    "\n",
    "def scale_max_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    scalemax = []\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "            \n",
    "                scale = np.max(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale'])\n",
    "                scalemax.append(scale)\n",
    "            \n",
    "    return scalemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalemax = scale_max_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 最大規模差值\n",
    "\n",
    "def scale_max_difference_interval():\n",
    "    \n",
    "    difference = np.array(scalemax)-np.array(GRinterval[3])\n",
    "            \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalemaxdifference = list(scale_max_difference_interval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 經度緯度偏向\n",
    "\n",
    "def lonlat_mean_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    longitude,latitude,i,maxindex,maxlen = [],[],0,0,0\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "                \n",
    "                maxindex = np.where(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale']==scalemax[i])[0][0]\n",
    "                \n",
    "                lon = data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['longitude'][maxindex+maxlen]\n",
    "                lat = data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['latitude'][maxindex+maxlen]\n",
    "                                                                                                    \n",
    "                longitude.append(lon)\n",
    "                latitude.append(lat)\n",
    "                \n",
    "                maxlen = maxlen+len(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale'])\n",
    "                                                                                                    \n",
    "                i = i+1\n",
    "            \n",
    "    return longitude,latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlatmax = lonlat_mean_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 深度偏向\n",
    "\n",
    "def depth_mean_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    depthmean,i,maxlen = [],0,0\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "                \n",
    "                maxindex = np.where(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale']==scalemax[i])[0][0]\n",
    "                \n",
    "                depth = data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['depth'][maxindex+maxlen]\n",
    "                depthmean.append(depth)\n",
    "                \n",
    "                maxlen = maxlen+len(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale'])\n",
    "                                                                                                    \n",
    "                i = i+1\n",
    "            \n",
    "    return depthmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthmax = depth_mean_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 區間能量累積\n",
    "\n",
    "def energy_sum_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    energysum = []\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "    \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "            \n",
    "                energy = sum(10**(data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale']*1.5+11.8-7))\n",
    "                energysum.append(energy)\n",
    "            \n",
    "    return energysum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "energysum = energy_sum_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 臨界規模\n",
    "\n",
    "def Critical_scale_interval(startyear,endyear,startmonth,endmonth):\n",
    "    \n",
    "    Criticalscale6,Criticalscale5,Criticalscale4 = [],[],[]\n",
    "    \n",
    "    years = np.arange(startyear,endyear)\n",
    "    months = np.arange(startmonth,endmonth)\n",
    "        \n",
    "    for year in years:\n",
    "        \n",
    "        daylist = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        if (year%4==0 and year%100!=0) or (year%400==0) : \n",
    "            \n",
    "            daylist = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "        for month in months:\n",
    "            \n",
    "            days = np.arange(1,daylist[month-1]+1)\n",
    "\n",
    "            for day in days:\n",
    "            \n",
    "                scale = data[(data['year']==year)&(data['month']==month)&(data['day']==day)]['scale']\n",
    "\n",
    "                if True in np.array(scale>=6):\n",
    "                    Critical6 = 1\n",
    "                else:\n",
    "                    Critical6 = 0\n",
    "\n",
    "                if True in np.array(scale>=5):\n",
    "                    Critical5 = 1\n",
    "                else:\n",
    "                    Critical5 = 0\n",
    "\n",
    "                if True in np.array(scale>=4):\n",
    "                    Critical4 = 1\n",
    "                else:\n",
    "                    Critical4 = 0\n",
    "\n",
    "                Criticalscale6.append(Critical6)\n",
    "                Criticalscale5.append(Critical5)\n",
    "                Criticalscale4.append(Critical4)\n",
    "            \n",
    "    return Criticalscale4,Criticalscale5,Criticalscale6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criticalscale = Critical_scale_interval(1999,2021,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 資料整合\n",
    "\n",
    "def Data_integration():\n",
    "    \n",
    "    TECdata = pd.DataFrame()\n",
    "    \n",
    "    TECdata['Criticalscale4'] = Criticalscale[0]\n",
    "    TECdata['Criticalscale5'] = Criticalscale[1]\n",
    "    TECdata['Criticalscale6'] = Criticalscale[2]\n",
    "    \n",
    "    TECdata['CriticalscaleM'] = scalemax\n",
    "    \n",
    "    TECdata['lon'] = lonlatmax[0]\n",
    "    TECdata['lat'] = lonlatmax[1]\n",
    "    TECdata['dapth'] = depthmax\n",
    "    \n",
    "    TECdata['GRa'] = GRinterval[0]\n",
    "    TECdata['GRb'] = GRinterval[1]\n",
    "    TECdata['GRloss'] = GRinterval[2]\n",
    "    \n",
    "    TECdata['scalemean'] = scalemean\n",
    "    TECdata['scalemax'] = scalemax\n",
    "    \n",
    "    TECdata['scalemaxexpected'] = GRinterval[3]\n",
    "    TECdata['scalemaxdifference'] = scalemaxdifference\n",
    "    \n",
    "    TECdata['energysum'] = energysum\n",
    "    \n",
    "    TECdata = pd.concat([TECdata,Probabilityday.iloc[:,:55]],axis=1)\n",
    "    \n",
    "    return TECdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 一週整合\n",
    "\n",
    "def OneWeek():\n",
    "        \n",
    "    X,C4,C5,C6,CM,lON,LAT,DAPTH,SM,maxindex,maxlen = [],[],[],[],[],[],[],[],[],0,0\n",
    "\n",
    "    for i in range(int(len(dataset)/7)):\n",
    "\n",
    "        X.append(np.mean(dataset.iloc[i*7:(i+1)*7,:]))\n",
    "        \n",
    "        C4.append(max(dataset.Criticalscale4.iloc[i*7:(i+1)*7]))\n",
    "        C5.append(max(dataset.Criticalscale5.iloc[i*7:(i+1)*7]))\n",
    "        C6.append(max(dataset.Criticalscale6.iloc[i*7:(i+1)*7]))\n",
    "        CM.append(max(dataset.CriticalscaleM.iloc[i*7:(i+1)*7]))\n",
    "        \n",
    "        maxindex = np.where(dataset.iloc[i*7:(i+1)*7].scalemax==max(dataset.iloc[i*7:(i+1)*7].scalemax))[0][0]\n",
    "        \n",
    "        lON.append(dataset['lon'][maxindex+maxlen])\n",
    "        LAT.append(dataset['lat'][maxindex+maxlen])\n",
    "        DAPTH.append(dataset['dapth'][maxindex+maxlen])\n",
    "        \n",
    "        SM.append(max(dataset.scalemax.iloc[i*7:(i+1)*7]))\n",
    "        \n",
    "        maxlen = maxlen+7\n",
    "\n",
    "    PartD = pd.DataFrame(X)\n",
    "\n",
    "    PartD.Criticalscale4 = C4\n",
    "    PartD.Criticalscale5 = C5\n",
    "    PartD.Criticalscale6 = C6\n",
    "    PartD.CriticalscaleM = CM\n",
    "    \n",
    "    PartD.lon = lON\n",
    "    PartD.lat = LAT\n",
    "    PartD.dapth = DAPTH\n",
    "    PartD.scalemax = SM\n",
    "        \n",
    "    return PartD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneWeekData = OneWeek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 兩週整合\n",
    "\n",
    "def TwoWeek():\n",
    "\n",
    "    X,C4,C5,C6,CM,lON,LAT,DAPTH,SM,maxindex,maxlen = [],[],[],[],[],[],[],[],[],0,0\n",
    "\n",
    "    for i in range(int(len(dataset)/14)):\n",
    "\n",
    "        X.append(np.mean(dataset.iloc[i*14:(i+1)*14,:]))\n",
    "        C4.append(max(dataset.Criticalscale4.iloc[i*14:(i+1)*14]))\n",
    "        C5.append(max(dataset.Criticalscale5.iloc[i*14:(i+1)*14]))\n",
    "        C6.append(max(dataset.Criticalscale6.iloc[i*14:(i+1)*14]))\n",
    "        CM.append(max(dataset.CriticalscaleM.iloc[i*14:(i+1)*14]))\n",
    "        \n",
    "        maxindex = np.where(dataset.iloc[i*14:(i+1)*14].scalemax==max(dataset.iloc[i*14:(i+1)*14].scalemax))[0][0]\n",
    "        \n",
    "        lON.append(dataset['lon'][maxindex+maxlen])\n",
    "        LAT.append(dataset['lat'][maxindex+maxlen])\n",
    "        DAPTH.append(dataset['dapth'][maxindex+maxlen])\n",
    "        \n",
    "        SM.append(max(dataset.scalemax.iloc[i*14:(i+1)*14]))\n",
    "        \n",
    "        maxlen = maxlen+14\n",
    "\n",
    "    PartD = pd.DataFrame(X)\n",
    "\n",
    "    PartD.Criticalscale4 = C4\n",
    "    PartD.Criticalscale5 = C5\n",
    "    PartD.Criticalscale6 = C6\n",
    "    PartD.CriticalscaleM = CM\n",
    "    \n",
    "    PartD.lon = lON\n",
    "    PartD.lat = LAT\n",
    "    PartD.dapth = DAPTH\n",
    "    PartD.scalemax = SM\n",
    "        \n",
    "    return PartD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoWeekData = TwoWeek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 三週整合\n",
    "\n",
    "def ThreeWeek():\n",
    "\n",
    "    X,C4,C5,C6,CM,lON,LAT,DAPTH,SM,maxindex,maxlen = [],[],[],[],[],[],[],[],[],0,0\n",
    "\n",
    "    for i in range(int(len(dataset)/21)):\n",
    "\n",
    "        X.append(np.mean(dataset.iloc[i*21:(i+1)*21,:]))\n",
    "        C4.append(max(dataset.Criticalscale4.iloc[i*21:(i+1)*21]))\n",
    "        C5.append(max(dataset.Criticalscale5.iloc[i*21:(i+1)*21]))\n",
    "        C6.append(max(dataset.Criticalscale6.iloc[i*21:(i+1)*21]))\n",
    "        CM.append(max(dataset.CriticalscaleM.iloc[i*21:(i+1)*21]))\n",
    "        \n",
    "        maxindex = np.where(dataset.iloc[i*21:(i+1)*21].scalemax==max(dataset.iloc[i*21:(i+1)*21].scalemax))[0][0]\n",
    "        \n",
    "        lON.append(dataset['lon'][maxindex+maxlen])\n",
    "        LAT.append(dataset['lat'][maxindex+maxlen])\n",
    "        DAPTH.append(dataset['dapth'][maxindex+maxlen])\n",
    "        \n",
    "        SM.append(max(dataset.scalemax.iloc[i*21:(i+1)*21]))\n",
    "        \n",
    "        maxlen = maxlen+21\n",
    "\n",
    "    PartD = pd.DataFrame(X)\n",
    "\n",
    "    PartD.Criticalscale4 = C4\n",
    "    PartD.Criticalscale5 = C5\n",
    "    PartD.Criticalscale6 = C6\n",
    "    PartD.CriticalscaleM = CM\n",
    "    \n",
    "    PartD.lon = lON\n",
    "    PartD.lat = LAT\n",
    "    PartD.dapth = DAPTH\n",
    "    PartD.scalemax = SM\n",
    "        \n",
    "    return PartD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreeWeekData = ThreeWeek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 四週整合\n",
    "\n",
    "def FourWeek():\n",
    "\n",
    "    X,C4,C5,C6,CM,lON,LAT,DAPTH,SM,maxindex,maxlen = [],[],[],[],[],[],[],[],[],0,0\n",
    "\n",
    "    for i in range(int(len(dataset)/28)):\n",
    "\n",
    "        X.append(np.mean(dataset.iloc[i*28:(i+1)*28,:]))\n",
    "        C4.append(max(dataset.Criticalscale4.iloc[i*28:(i+1)*28]))\n",
    "        C5.append(max(dataset.Criticalscale5.iloc[i*28:(i+1)*28]))\n",
    "        C6.append(max(dataset.Criticalscale6.iloc[i*28:(i+1)*28]))\n",
    "        CM.append(max(dataset.CriticalscaleM.iloc[i*28:(i+1)*28]))\n",
    "        \n",
    "        maxindex = np.where(dataset.iloc[i*28:(i+1)*28].scalemax==max(dataset.iloc[i*28:(i+1)*28].scalemax))[0][0]\n",
    "        \n",
    "        lON.append(dataset['lon'][maxindex+maxlen])\n",
    "        LAT.append(dataset['lat'][maxindex+maxlen])\n",
    "        DAPTH.append(dataset['dapth'][maxindex+maxlen])\n",
    "        \n",
    "        SM.append(max(dataset.scalemax.iloc[i*28:(i+1)*28]))\n",
    "        \n",
    "        maxlen = maxlen+28\n",
    "\n",
    "    PartD = pd.DataFrame(X)\n",
    "\n",
    "    PartD.Criticalscale4 = C4\n",
    "    PartD.Criticalscale5 = C5\n",
    "    PartD.Criticalscale6 = C6\n",
    "    PartD.CriticalscaleM = CM\n",
    "    \n",
    "    PartD.lon = lON\n",
    "    PartD.lat = LAT\n",
    "    PartD.dapth = DAPTH\n",
    "    PartD.scalemax = SM\n",
    "        \n",
    "    return PartD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "FourWeekData = FourWeek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 儲存資料\n",
    "\n",
    "def Store_data():\n",
    "    \n",
    "    filePath = 'D:\\Dataset\\Earthquake\\Calculus/1_CWB.xlsx'\n",
    "    dataset.to_excel(filePath,index=False)\n",
    "    \n",
    "    filePath = 'D:\\Dataset\\Earthquake\\Calculus/7_CWB.xlsx'\n",
    "    OneWeekData.to_excel(filePath,index=False)\n",
    "    \n",
    "    filePath = 'D:\\Dataset\\Earthquake\\Calculus/14_CWB.xlsx'\n",
    "    TwoWeekData.to_excel(filePath,index=False)\n",
    "    \n",
    "    filePath = 'D:\\Dataset\\Earthquake\\Calculus/21_CWB.xlsx'\n",
    "    ThreeWeekData.to_excel(filePath,index=False)\n",
    "\n",
    "    filePath = 'D:\\Dataset\\Earthquake\\Calculus/28_CWB.xlsx'\n",
    "    FourWeekData.to_excel(filePath,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 793.977364,
   "position": {
    "height": "40px",
    "left": "80px",
    "right": "20px",
    "top": "115.96px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
